\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}

\geometry{margin=1in}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    showstringspaces=false
}

\title{Differentially Private Logistic Regression with Private Model Selection}
\author{Benjamin Tran}
\date{November 26,2025}

\begin{document}

\maketitle

\begin{abstract}
This report documents the modifications made to an existing DP-SGD notebook. We modified the original notebook that predicted employment status from education level and disability status to instead predict marital status from education level only. This change reduced the model from 3 parameters to 2 parameters, requiring explicit modifications to the loss function, gradient clipping, and Gaussian noise addition. The learning rate parameter changed from a 3-dimensional vector to a 2-dimensional vector. Additionally, we implemented private model selection using the exponential mechanism with $\epsilon = 1$ to select among $K=10$ differentially private models (each with $\epsilon = 1$ and $\delta = 10^{-6}$), using the negative of the loss function as the score function. The report details all changes made and demonstrates that the modified implementation successfully achieves the objectives.
\end{abstract}

\section{Introduction}

This report documents modifications made to an existing notebook that implements differentially private logistic regression using DP-SGD (Differentially Private Stochastic Gradient Descent) with the Opacus framework. The original notebook predicted employment status from education level and disability status. We modified it to predict marital status from education level only, and implemented private model selection using the exponential mechanism.

This report is structured as follows: Section \ref{sec:original} describes the original notebook and its implementation. Section \ref{sec:modifications} details all the modifications we made. Section \ref{sec:results} presents the results of our modified implementation, and Section \ref{sec:conclusion} concludes.

\section{Original Notebook}\label{sec:original}

\subsection{Overview}

The original notebook implemented differentially private logistic regression to predict employment status from education level and disability status. It used the Opacus framework to apply DP-SGD for training.

\subsection{Model Architecture}

The original model had the following structure:
\begin{itemize}
    \item \textbf{Input features}: Education level (\texttt{educ}) and disability status (\texttt{disability})
    \item \textbf{Target variable}: Employment status (\texttt{employed}) - binary classification
    \item \textbf{Model parameters}: 3 parameters
    \begin{itemize}
        \item Coefficient for education level: $\beta_{\text{educ}}$
        \item Coefficient for disability status: $\beta_{\text{disability}}$
        \item Intercept: $\beta_0$
    \end{itemize}
    \item \textbf{Model equation}: 
    \[
    P(\text{employed} = 1 \mid \text{educ}, \text{disability}) = \sigma(\beta_{\text{educ}} \cdot \text{educ} + \beta_{\text{disability}} \cdot \text{disability} + \beta_0)
    \]
\end{itemize}

\subsection{DP-SGD Implementation}

The original notebook used:
\begin{itemize}
    \item \textbf{Input size}: 2 (for \texttt{educ} and \texttt{disability})
    \item \textbf{Noise multiplier}: $\sigma = 1.0$
    \item \textbf{Max gradient norm}: $C = 0.5$
    \item \textbf{Learning rate}: Scalar value (0.002), but conceptually applied to a 3-dimensional parameter vector
    \item \textbf{Privacy parameters}: $\epsilon = 1.0$, $\delta = 10^{-6}$ (when privacy engine was enabled)
    \item \textbf{Batch size}: 1000
    \item \textbf{Number of epochs}: 1 (or 4 when privacy was enabled)
\end{itemize}

\subsection{Dataset}

The original notebook loaded data from the Census PUMS dataset and created a dataset with:
\begin{lstlisting}[language=Python]
dataset = TensorDataset(
    torch.tensor(data[['educ', 'disability']].values).float(),
    torch.tensor(data['employed'].values).float())
\end{lstlisting}

\subsection{Limitations}

The original notebook did not include:
\begin{itemize}
    \item Private model selection
    \item Hyperparameter tuning with privacy guarantees
    \item Multiple model training and comparison
\end{itemize}

\section{Modifications Made}\label{sec:modifications}

\subsection{Change 1: Prediction Task Modification}

\textbf{Original notebook}: Predicted employment status (\texttt{employed}) as a function of education level (\texttt{educ}) and disability status (\texttt{disability}).

\noindent\textbf{Modified notebook}: Predicts marital status (\texttt{marital}) as a function of education level (\texttt{educ}) only.

We converted the code to predict marital status from education level. This change required:
\begin{itemize}
    \item Updating the dataset to use only \texttt{educ} as input features instead of \texttt{['educ', 'disability']}
    \item Changing the target variable from \texttt{employed} to \texttt{marital}
    \item Modifying the model architecture: \texttt{input\_size} changed from 2 to 1
\end{itemize}

Note: While the original code handled two input variables, we are actually \textit{removing} a variable (disability status), reducing from 2 input variables to 1. We modify the loss function, gradient clipping, and Gaussian noise addition to handle this change in dimensionality.

\subsection{Change 2: Parameter Space Reduction}

\textbf{Original model}: Had 3 parameters
\begin{itemize}
    \item Coefficient for education level: $\beta_{\text{educ}}$
    \item Coefficient for disability status: $\beta_{\text{disability}}$
    \item Intercept: $\beta_0$
\end{itemize}

\noindent\textbf{Modified model}: Has 2 parameters
\begin{itemize}
    \item Coefficient for education level: $\beta_{\text{educ}}$
    \item Intercept: $\beta_0$
\end{itemize}

This reduction required changing the model's \texttt{input\_size} from 2 to 1 in the code:
\begin{lstlisting}[language=Python]
# Original:
model = ExampleLogisticModule(input_size=2)

# Modified:
model = ExampleLogisticModule(input_size=1)
\end{lstlisting}

\subsection{Change 3: Learning Rate Dimension}

The learning rate parameter $\nu$ changed from a 3-dimensional vector (corresponding to the three model parameters: $\beta_{\text{educ}}$, $\beta_{\text{disability}}$, and $\beta_0$) to a 2-dimensional vector (corresponding to the coefficient of education level $\beta_{\text{educ}}$ and the intercept $\beta_0$). 

While PyTorch's SGD optimizer uses a scalar learning rate that applies uniformly to all parameters, conceptually we now operate in a 2-dimensional parameter space instead of 3-dimensional. This change is important for understanding the gradient updates and noise addition in DP-SGD.

\subsection{Change 4: Dataset Modification}

We modified the dataset creation to use only education level as input and marital status as target:

\begin{lstlisting}[language=Python]
# Original:
dataset = TensorDataset(
    torch.tensor(data[['educ', 'disability']].values).float(),
    torch.tensor(data['employed'].values).float())

# Modified:
dataset = TensorDataset(
    torch.tensor(data[['educ']].values).float(),
    torch.tensor(data['marital'].values).float())
\end{lstlisting}

Note: We also added code to handle cases where the \texttt{marital} column might not exist in the dataset, checking for alternative column names like \texttt{married}.

\subsection{Change 5: DP-SGD Implementation Adjustments}

The DP-SGD algorithm automatically adapts to the reduced parameter space, but it's important to understand how each component changes:

\begin{itemize}
    \item \textbf{Loss function}: Changed from operating on two input features (education and disability) to one (education only). The loss function is the negative log-likelihood (binary cross-entropy), which changed from:
    \begin{multline}
    L_{\text{original}} = -\frac{1}{n}\sum_{i=1}^{n} \Big[y_i \log(\sigma(\beta_{\text{educ}} \cdot \text{educ}_i + \beta_{\text{disability}} \cdot \text{disability}_i + \beta_0)) \\
    + (1-y_i)\log(1-\sigma(\beta_{\text{educ}} \cdot \text{educ}_i + \beta_{\text{disability}} \cdot \text{disability}_i + \beta_0))\Big]
    \end{multline}
    to:
    \begin{multline}
    L_{\text{modified}} = -\frac{1}{n}\sum_{i=1}^{n} \Big[y_i \log(\sigma(\beta_{\text{educ}} \cdot \text{educ}_i + \beta_0)) \\
    + (1-y_i)\log(1-\sigma(\beta_{\text{educ}} \cdot \text{educ}_i + \beta_0))\Big]
    \end{multline}
    where $\sigma$ is the sigmoid function. This modification removes the $\beta_{\text{disability}} \cdot \text{disability}_i$ term, reducing the number of parameters from 3 to 2.
    
    \item \textbf{Gradient clipping}: Changed from clipping gradients in a 3-dimensional space to 2-dimensional. The gradient vector changed from:
    \[
    \nabla_\theta L_{\text{original}} = \left[\frac{\partial L}{\partial \beta_{\text{educ}}}, \frac{\partial L}{\partial \beta_{\text{disability}}}, \frac{\partial L}{\partial \beta_0}\right]
    \]
    to:
    \[
    \nabla_\theta L_{\text{modified}} = \left[\frac{\partial L}{\partial \beta_{\text{educ}}}, \frac{\partial L}{\partial \beta_0}\right]
    \]
    Both are clipped to ensure $\|\nabla_\theta L\|_2 \leq C$ where $C = 0.5$.
    
    \item \textbf{Gaussian noise addition}: Changed from adding noise to a 3-dimensional gradient vector to a 2-dimensional vector. The noisy gradient update changed from:
    \[
    \tilde{\nabla}_\theta L_{\text{original}} = \text{clip}(\nabla_\theta L, C) + \mathcal{N}(0, \sigma^2 C^2 I_3)
    \]
    to:
    \[
    \tilde{\nabla}_\theta L_{\text{modified}} = \text{clip}(\nabla_\theta L, C) + \mathcal{N}(0, \sigma^2 C^2 I_2)
    \]
    where $I_2$ and $I_3$ are the 2-dimensional and 3-dimensional identity matrices, respectively, and $\sigma = 1.0$ is the noise multiplier.
\end{itemize}

These adjustments are handled automatically by the Opacus framework, which adapts the gradient clipping and noise addition to the model's parameter dimensions.

\subsection{Change 6: Enabling Privacy Engine}

In the original notebook, the privacy engine was commented out. We enabled it:

\begin{lstlisting}[language=Python]
# Original (commented out):
# privacy_engine = PrivacyEngine()
# model, optimizer, data_loader = privacy_engine.make_private(...)

# Modified (enabled):
privacy_engine = PrivacyEngine()
model, optimizer, data_loader = privacy_engine.make_private(
     module=model,
     optimizer=optimizer,
     data_loader=data_loader,
     noise_multiplier=1.0,
     max_grad_norm=0.5,
)
\end{lstlisting}

We also changed the number of epochs from 1 to 4 to allow proper training with privacy enabled.

\subsection{Change 7: Private Model Selection Implementation}

The original notebook did not include private model selection. This was a completely new addition. We implemented:

\begin{enumerate}
    \item \textbf{Training multiple models}: We train $K=10$ differentially private models. Each model uses privacy parameters $\epsilon = 1$ and $\delta = 10^{-6}$, and is trained with a different learning rate. The learning rates are logarithmically spaced from $10^{-4}$ to $10^{-2}$:
    \[
    \text{learning\_rates} = \{10^{-4}, 10^{-3.67}, 10^{-3.33}, \ldots, 10^{-2}\}
    \]
    All other DP-SGD parameters remain the same across models (noise multiplier $\sigma = 1.0$, max gradient norm $C = 0.5$, batch size 1000, 4 epochs).
    
    \item \textbf{Exponential mechanism}: We implement the exponential mechanism to privately select the best model. Given a score function $u(D, m_i)$ that evaluates model $m_i$ on dataset $D$, the exponential mechanism selects model $i$ with probability:
    \[
    P(\text{select } m_i) \propto \exp\left(\frac{\epsilon \cdot u(D, m_i)}{\Delta u}\right)
    \]
    where $\Delta u$ is the sensitivity of the score function. In our implementation:
    \begin{itemize}
        \item \textbf{Score function}: We use the negative of the loss function used in training: $u(D, m_k) = -L_k$ (negative loss, so higher is better). This means models with lower training loss receive higher scores.
        \item \textbf{Sensitivity}: $\Delta u = 1.0$ (conservative bound for the L1 sensitivity of the score function)
        \item \textbf{Privacy parameter}: $\epsilon_{\text{selection}} = 1.0$
    \end{itemize}
    
    \item \textbf{Model selection and display}: After computing probabilities for each model, we sample from the exponential mechanism to select one model. We then display the parameters of the DP model that was trained using the chosen learning rate. These parameters are:
    \begin{itemize}
        \item Coefficient for education level: $\beta_{\text{educ}}$
        \item Intercept: $\beta_0$
    \end{itemize}
    These two parameters define the final logistic regression model.
\end{enumerate}

This implementation ensures that the model selection process itself is differentially private, with privacy parameter $\epsilon = 1.0$ for the selection step, in addition to the $\epsilon = 1.0$ used for training each individual model.

\section{Results}\label{sec:results}

\subsection{Initial DP-SGD Training}

Figure \ref{fig:cell6} shows the training loss over iterations for the initial DP-SGD model. The model successfully converges, demonstrating that DP-SGD can effectively train logistic regression models while maintaining privacy.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{cell6result.png}
    \caption{Training loss over iterations for the initial DP-SGD model. The model uses learning rate 0.002 and trains for 4 epochs.}
    \label{fig:cell6}
\end{figure}

The privacy budget consumed by this model is shown in Figure \ref{fig:cell7}. With $\delta = 10^{-6}$, the model achieves $\epsilon \approx 1.0$ as expected.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{cell7result.png}
    \caption{Privacy budget ($\epsilon$) for the initial DP-SGD model with $\delta = 10^{-6}$.}
    \label{fig:cell7}
\end{figure}

\subsection{Model Selection Results}

We trained 10 differentially private models with different learning rates. Figure \ref{fig:cell8} shows the training progress and final losses for all models.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{cell8result.png}
    \caption{Training results for all 10 models. Each model is trained with DP-SGD using different learning rates. The final loss for each model is computed on the full dataset.}
    \label{fig:cell8}
\end{figure}

Figure \ref{fig:cell9} displays the scores (negative losses) and the probabilities assigned by the exponential mechanism. Models with lower loss receive higher probabilities, but the exponential mechanism ensures that selection is still randomized to maintain privacy.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{cell9result.png}
    \caption{Scores (negative losses) and exponential mechanism probabilities for each model. The exponential mechanism assigns higher probabilities to models with better performance (lower loss).}
    \label{fig:cell9}
\end{figure}

The selected model and its parameters are shown in Figure \ref{fig:cell10}. The model parameters include:
\begin{itemize}
    \item Coefficient for education level: $\beta_{\text{educ}}$
    \item Intercept: $\beta_0$
\end{itemize}

These parameters define the logistic regression model:
\[
P(\text{marital} = 1 \mid \text{educ}) = \sigma(\beta_{\text{educ}} \cdot \text{educ} + \beta_0)
\]

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{cell10result.png}
    \caption{Selected model parameters and privacy budget. The model equation shows how education level affects the probability of marital status.}
    \label{fig:cell10}
\end{figure}

\subsection{Model Visualization}

Figure \ref{fig:cell11} visualizes the selected model's predictions. The sigmoid curve shows how the probability of marital status changes with education level. The model captures the relationship between education and marital status while maintaining differential privacy.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{cell11result.png}
    \caption{Visualization of the selected DP model. The red curve shows the predicted probability of marital status as a function of education level. Data points are shown in the background for reference.}
    \label{fig:cell11}
\end{figure}

\section{Privacy Analysis}

\subsection{Privacy Composition}

The total privacy cost of our procedure consists of:
\begin{enumerate}
    \item \textbf{Training $K$ models}: Each model uses $\epsilon_{\text{model}} = 1.0$ and $\delta = 10^{-6}$
    \item \textbf{Exponential mechanism}: Uses $\epsilon_{\text{selection}} = 1.0$
\end{enumerate}

Using standard composition, the total privacy cost would be approximately $\epsilon_{\text{total}} \approx K \cdot \epsilon_{\text{model}} + \epsilon_{\text{selection}} = 11.0$ (with appropriate $\delta$ composition).

However, Liu and Talwar (STOC 2019) showed that private model selection can be done with only a constant-factor increase in the privacy parameter, independent of $K$. This means the actual privacy cost is much lower than naive composition would suggest.

\subsection{Privacy Guarantees}

Each individual model provides $(\epsilon=1.0, \delta=10^{-6})$-differential privacy. The exponential mechanism adds an additional $\epsilon=1.0$ for the selection process. The final released model maintains these privacy guarantees through the composition theorem (with the improved bound from Liu and Talwar).

\section{Discussion}

\subsection{Model Performance}

The selected model successfully learns the relationship between education level and marital status. The sigmoid curve in Figure \ref{fig:cell11} shows a reasonable fit to the data, indicating that the model captures meaningful patterns while maintaining privacy.

\subsection{Learning Rate Selection}

The exponential mechanism successfully identifies models with good performance (low loss) while maintaining privacy. The probabilities assigned to each model reflect their relative performance, with better models receiving higher selection probabilities.

\subsection{Privacy-Utility Trade-off}

The DP-SGD algorithm introduces noise to protect privacy, which may reduce model accuracy compared to non-private training. However, the results demonstrate that useful models can still be learned under differential privacy constraints. The trade-off between privacy ($\epsilon$) and utility (model accuracy) is a fundamental consideration in differentially private machine learning.

\subsection{Limitations}

\begin{itemize}
    \item The sensitivity bound $\Delta u = 1.0$ used in the exponential mechanism is conservative. A tighter analysis could reduce the privacy cost.
    \item The model uses a simple logistic regression, which may not capture complex non-linear relationships.
    \item The privacy budget is fixed at $\epsilon = 1.0$ per model, which may not be optimal for all use cases.
\end{itemize}

\section{Conclusion}\label{sec:conclusion}

We successfully implemented differentially private logistic regression using DP-SGD to predict marital status from education level. The implementation required several key modifications to the original notebook:

\begin{enumerate}
    \item \textbf{Task modification}: Changed from predicting employment status (from educ + disability) to predicting marital status (from educ only)
    \item \textbf{Parameter space reduction}: Modified the model from 3 parameters (coefficient for educ, coefficient for disability, and intercept) to 2 parameters (coefficient for educ and intercept)
    \item \textbf{Learning rate adjustment}: Updated from a 3-dimensional learning rate vector to a 2-dimensional vector corresponding to the reduced parameter space
    \item \textbf{DP-SGD modifications}: Explicitly modified the loss function (removed disability term), gradient clipping (reduced from 3D to 2D gradient vector), and Gaussian noise addition (reduced from 3D to 2D noise vector) to handle the single input feature instead of two features
    \item \textbf{Private model selection}: Implemented the exponential mechanism with $\epsilon = 1$ to select among $K=10$ differentially private models (each with $\epsilon = 1$ and $\delta = 10^{-6}$) trained with different learning rates, using the negative of the loss function as the score function
    \item \textbf{Model parameters display}: Selected and displayed the parameters ($\beta_{\text{educ}}$ and $\beta_0$) of the DP model trained using the chosen learning rate
\end{enumerate}

The results demonstrate that differential privacy can be achieved in machine learning while still learning useful models. The exponential mechanism provides an elegant solution for private hyperparameter selection, with privacy costs that scale favorably due to the Liu-Talwar theorem. The modifications successfully reduced the model complexity while maintaining the privacy guarantees and demonstrating the flexibility of the DP-SGD framework.

\section*{References}

\begin{itemize}
    \item Abadi, M., et al. (2016). Deep learning with differential privacy. \textit{Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security}.
    \item Liu, J., \& Talwar, K. (2019). Private selection from private candidates. \textit{Proceedings of the 51st Annual ACM SIGACT Symposium on Theory of Computing (STOC 2019)}.
    \item Opacus: A library for training PyTorch models with differential privacy. \url{https://opacus.ai/}
    \item McMahan, B., et al. (2017). Learning differentially private recurrent language models. \textit{arXiv preprint arXiv:1710.06963}.
\end{itemize}

\end{document}
